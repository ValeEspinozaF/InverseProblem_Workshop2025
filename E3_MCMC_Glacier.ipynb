{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics and Inverse Problems 2025\n",
    "## Exercise 3 -  Glacier gravity profile\n",
    "\n",
    "### Author(s), contact(s), and dates:\n",
    "- Author: Valentina Espinoza Fern√°ndez (University of Copenhagen), Klaus Mosegaard (Niels Bohr Institute)\n",
    "- Email:  vf@ign.ku.dk\n",
    "- Date:   10th of January 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabel of contents\n",
    "* [Problem setup](#problem-setup)\n",
    "* [Discretized Problem](#discretized-problem)\n",
    "* [Markov Chain Monte Carlo](#mcmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will work out the depth profile of a glacier from gravity measurements. We will use a Markov Chain Monte Carlo method to iteratively sample the posterior distribution. We will do so by using the Metropolis-Hastings algorithm to accept or reject the proposed models sampled from the prior distribution, based on their likelihood (fit) against the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public dependencies\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local dependencies - YOUR COMPUTER\n",
    "from DEPENDENCIES.e3_dependencies import plot_stations, set_figure, valley_parabola, plot_valley_outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup <a  class=\"anchor\" id=\"problem-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the observed gravity data\n",
    "\n",
    "gravdata = np.loadtxt(\"DATA/gravdata.txt\")\n",
    "x_obs = gravdata[:, 0]           # horizontal coordinate for each observation (in m)\n",
    "d_obs = gravdata[:, 1] * 1e-5    # horizontal gravity anomaly (in m/s2)\n",
    "n_stat = len(d_obs)              # Number of stations of observation\n",
    "\n",
    "# Data covariance matrix\n",
    "std_d = 1.2 * 1e-5                              # data standard deviation (in m/s2)\n",
    "cov_d = np.identity(n_stat) * std_d**2          # data covariance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the gravity data\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.grid(lw=0.5, alpha=0.5)\n",
    "ax.errorbar(x_obs, d_obs, std_d, capsize=2, fmt=\"o\", elinewidth=0.5)\n",
    "ax.set(xlabel = 'x-coordinate [m]',\n",
    "       ylabel = 'gravity anomaly [$m/s^2$]',\n",
    "       title = 'Measured horizontal gravity anomaly'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some model parameters\n",
    "\n",
    "x0 = 425            # leftmost outcrop of the valley\n",
    "xn = 3000           # rightmost outcrop of the valley\n",
    "d_density = -1733   # density contrast (kg/m3)\n",
    "G_grav = 6.67e-11   # gravitational constant (in m3 kg-1 s-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretized Model <a  class=\"anchor\" id=\"discretized-problem\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation between gravity anomaly data point $\\Delta g_j$ and the model parameter $h(x)$ is given by:\n",
    "$$ \\Delta g_j = \\Delta g(x_j) = G\\Delta\\rho \\int_0^a ln \\left( \\frac{(x-x_j)^2 + h(x)^2}{(x-x_j)^2}\\right) dx $$\n",
    "\n",
    "which can be discretized into M rectangles with width $\\Delta x$ and midpoints at $x_i$:\n",
    "\n",
    "$$ \\Delta g_j = G\\Delta\\rho \\sum_{i=1}^M ln \\left( \\frac{(x_i-x_j)^2 + h(x_i)^2}{(x_i-x_j)^2 + \\delta} \\right) \\Delta x $$\n",
    "\n",
    "where the inclusion of $\\delta$ prevents numerical problems in the denominator of the logarithm.\n",
    "\n",
    "1. Is this problem linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate g(m) from the given array of thicknesses (m0).\n",
    "def calculate_g_m(m0, m0_x, x_obs, x_width):\n",
    "    delta = 1e-15\n",
    "    grav_l = np.zeros(n_stat)\n",
    "    \n",
    "    for j in range(n_stat):\n",
    "        \n",
    "        grav_anom = 0\n",
    "        for l in range(len(m0_x)): \n",
    "            grav_anom += G_grav * d_density * np.log(( (m0_x[l] - x_obs[j])**2 + m0[l]**2 ) / ((m0_x[l] - x_obs[j])**2 + delta)) * x_width\n",
    "            \n",
    "        grav_l[j] = grav_anom\n",
    "        \n",
    "    return grav_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take as a preferred model a parabola-shaped valley which contains the ice-sheet. Of our choice is the amount of model parameters, which translates into the amount of *columns* with which we will attempt to model the valley. The more columns (`n_model`) you use, the easier will be to fit the simulated data, but the computation will take longer. A suggested value is **18**, but do see how the valley and simulated data changes when tweaking this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_model = 18\n",
    "\n",
    "m0, m0_x, m0_std, x_width = valley_parabola(n_model, x0, xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot initial (prior) model \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,6), gridspec_kw={\"wspace\":0.3})\n",
    "\n",
    "# Plot ice-sheet thickness (m - model parameters)\n",
    "plot_valley_outline(ax1, m0_x, m0, x_width, label='valley outline')\n",
    "ax1.errorbar(m0_x, -m0, m0_std, capsize=2, fmt=\"o\", color='r', elinewidth=0.5, label=\"preferred model\")\n",
    "plot_stations(ax1, x_obs)\n",
    "set_figure(ax1, m0_x[0], m0_x[-1])\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "# Plot simulated vs observed gravity anomaly (d - data)\n",
    "ax2.plot(np.arange(1, n_stat+1), calculate_g_m(m0, m0_x, x_obs, x_width), 'ob', label=\"g(m) from initial proposal\")\n",
    "ax2.errorbar(np.arange(1, n_stat+1), d_obs, std_d, capsize=2, fmt=\"s\", color='r', elinewidth=0.5, label=\"d_observed\")\n",
    "ax2.set(xticks = np.arange(1, n_stat+1),\n",
    "       title = \"Observed vs modelled gravity anomaly\",\n",
    "       xlabel = \"Observation station\",\n",
    "       ylabel = \"Gravity anomaly [m/s-2]\");\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try `n_model = 3` and re-run the above cells. Can you predict the pros and cons of few vs many model parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo  <a  class=\"anchor\" id=\"mcmc\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the the amount of iteration for the MCMC random walk. To test that the acceptance rate is whithin the appropiate range (30-70%), we may start with a low number of iterations (say 10000, `n_size = int(1e4)`). For the final run we can increase this number to `5e4`. This is still low for a MCMC, which would usually do millions iterations, but that would take too long time for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_size = int(1e4)   # size of the model space (number of iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood expression is given by:\n",
    "$$ L(m) = \\textrm{exp} \\left( -\\frac{1}{2}(d - g(m))^T C_d^{-1}(d - g(m)) \\right)$$\n",
    "\n",
    "where $C_d$ is the data (noise) covariance matrix, and g(m) is the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the likelihood of the simulated data given the observed data\n",
    "def likelihood(d_obs, g_m):\n",
    "    const = 1\n",
    "    diff = d_obs - g_m\n",
    "    return const * np.exp(-1/2 * np.dot(np.dot(diff.T, np.linalg.inv(cov_d)), diff))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will employ a Metropolis algorithm, like so: \n",
    "\n",
    "At each step we will propose a new model by randomly perturbing one model parameter by assigning it a new value according to the prior. If the likelihood ($L(m)$) of this new model improves, we keep it as the new current model. If not, we will randomly accept/reject the model against a uniform distribution. This will improve our chance of sampling the whole posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_m_rand = np.random.randint(0, n_model, n_size)       # ensemble of model parameter indices to change at each iteration\n",
    "u_rand = np.random.uniform(0, 1, n_size)               # ensemble of vs to accept against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturb the current model, by changing a given model parameter (m0[i_m]) to a new value sampled from the prior (np.random.normal(m0[i_m], m0_std[i_m])).\n",
    "\n",
    "def perturb_model(m0, m0_std, m_cur, i_m):\n",
    "    new_m = copy.deepcopy(m_cur)\n",
    "    new_m[i_m] = np.random.normal(m0[i_m], m0_std[i_m])\n",
    "    return new_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Solutions are therefore sampled at a rate proportional to their *a posteriori* probabilities, that is, models consistent with *a priori* information as well as observations are picked most often, whereas models that are in incompatible with either *a priori* information or observations (or both) are\n",
    "rarely sampled."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup empty arrays to hold iterative data\n",
    "m_space = np.zeros((n_model, n_size))\n",
    "g_m_space = np.zeros((n_stat, n_size))\n",
    "accept_ratio = np.zeros(n_size)\n",
    "likelihood_space = np.zeros(n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "\n",
    "# Set off with m0 as the current model\n",
    "m_current = m0     \n",
    "g_m_current = calculate_g_m(m_current, m0_x, x_obs, x_width)\n",
    "L_current = likelihood(d_obs, g_m_current)\n",
    "\n",
    "n_accept = 0       # accepted models count\n",
    "\n",
    "for i in range(n_size):\n",
    "    \n",
    "    # Perturb current model\n",
    "    m_perturb = perturb_model(m0, m0_std, m_current, i_m_rand[i])\n",
    "    g_m_perturb = calculate_g_m(m_perturb, m0_x, x_obs, x_width)\n",
    "    L_perturb = likelihood(d_obs, g_m_perturb)\n",
    "\n",
    "\n",
    "    # Accept/reject perturbed model\n",
    "    if u_rand[i] < L_perturb/L_current:\n",
    "        \n",
    "        # Update to new model\n",
    "        m_current = m_perturb\n",
    "        g_m_current = g_m_perturb\n",
    "        L_current = L_perturb\n",
    "        \n",
    "        n_accept += 1\n",
    "\n",
    "\n",
    "    # Store current model space\n",
    "    m_space[:, i] = m_current\n",
    "    g_m_space[:, i] = g_m_current\n",
    "    likelihood_space[i] = L_current\n",
    "    accept_ratio[i] = n_accept/(i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"P_accepted percentage: \", n_accept/n_size * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further diagnostics, we want to plot the evolution of the acceptance ratio and likelihood with each subsequent iteration. A reasonable run should show:\n",
    "- An acceptance rate that quickly progressing  that eventually stalls, and\n",
    "- A likelihood (or log-likelihood) that approaches zero but may wander around low values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot walk diagnostics\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize =(14, 4))\n",
    "\n",
    "# Acceptance rate\n",
    "ax1.plot(np.arange(0, n_size), accept_ratio)\n",
    "ax1.grid(lw=0.5, alpha=0.5)\n",
    "ax1.set(ylabel = \"Acceptance rate\", \n",
    "        xlabel = \"Iteration\",\n",
    "        title = \"Acceptance rate evolution\",\n",
    "        ylim = [-0.05, 1.05],\n",
    ");\n",
    "\n",
    "\n",
    "# Data fit\n",
    "ax2.plot(np.arange(0, n_size), np.log(likelihood_space))\n",
    "ax2.grid(lw=0.5, alpha=0.5)\n",
    "ax2.set(ylabel = \"log(L(m))\", \n",
    "        xlabel = \"Iteration\",\n",
    "        title = \"Log(likelihood) evolution\",\n",
    "        ylim=(-40, 0)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plots above, we determine that the first 1000 values are what is called the burn-in, i.e., the part of the walk before reaching the low-misfit area of the posterior. These are discarted from the final data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_in = 1000      # transient iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, we may visualize the posterior distribution of model parameters and the simulated data that comes from it. For easiness, we will only plot the mean and standard deviation of the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_m_data = pd.DataFrame()\n",
    "g_m_data[\"station\"] = np.arange(1, n_stat+1)\n",
    "\n",
    "for index, row in g_m_data.iterrows():\n",
    "    \n",
    "    rect_data = [g_m_space[index,j] for j in range(n_size)][burn_in:]\n",
    "    g_m_data.loc[index, \"mean\"] = np.mean(rect_data)\n",
    "    g_m_data.loc[index, \"std\"] = np.std(rect_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulated vs observed gravity anomaly (d - data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize =(7,5))\n",
    "\n",
    "# Observed data\n",
    "ax.errorbar(np.arange(1, n_stat+1), d_obs, std_d, capsize=2, fmt=\"s\", color='r', elinewidth=0.5, label=\"d_observed\")\n",
    "\n",
    "# Initial model proposal (prior)\n",
    "ax.plot(np.arange(1, n_stat+1), calculate_g_m(m0, m0_x, x_obs, x_width), 'ob', alpha=0.2, label=\"g(m) from initial proposal\")\n",
    "\n",
    "# g(m) from posterior distribution\n",
    "ax.errorbar(g_m_data[\"station\"], g_m_data[\"mean\"], g_m_data[\"std\"], capsize=2, fmt=\"^\", color='g', elinewidth=0.5, label=\"g(m) from posterior (N=%s)\" %n_size)\n",
    "\n",
    "ax.set(xticks = np.arange(1, n_stat+1),\n",
    "       title = \"Observed vs modelled gravity anomaly\",\n",
    "       xlabel = \"Observation station\",\n",
    "       ylabel = \"Gravity anomaly [m/s-2]\");\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_data = pd.DataFrame()\n",
    "posterior_data[\"column\"] = np.arange(1, n_model+1)\n",
    "\n",
    "for index, row in posterior_data.iterrows():\n",
    "    \n",
    "    rect_data = [m_space[index,j] for j in range(n_size)][burn_in:]\n",
    "    posterior_data.loc[index, \"mean\"] = np.mean(rect_data)\n",
    "    posterior_data.loc[index, \"std\"] = np.std(rect_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ice-sheet thickness (m - model parameters)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "plot_valley_outline(ax, m0_x, m0, x_width, label='initial outline')\n",
    "plot_valley_outline(ax, m0_x, posterior_data[\"mean\"], x_width, label='final outline', linestyle='-')\n",
    "ax.errorbar(m0_x, -posterior_data[\"mean\"], posterior_data[\"std\"], capsize=2, fmt=\"none\", ecolor='r', elinewidth=0.5)\n",
    "\n",
    "plot_stations(ax, x_obs)\n",
    "set_figure(ax, m0_x[0], m0_x[-1])\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do have the full space of models, we may plot the frequency of thicknesses for the discretized ice-sheet columns (posterior distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot frequency of thickness for each column (meant for 18 columns!)\n",
    "\n",
    "fig, ax = plt.subplots(2, 9, figsize =(22,7), gridspec_kw=dict(wspace=0.2, hspace=0.4))\n",
    "axes = [(0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8),\n",
    "        (1,0), (1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (1,7), (1,8)]\n",
    "\n",
    "\n",
    "for n in range(n_model):\n",
    "    data = [m_space[n,j] for j in range(n_size)][burn_in:]\n",
    "    ax[axes[n]].grid(lw=0.5, alpha=0.5)\n",
    "    ax[axes[n]].hist(data, bins=50)\n",
    "    ax[axes[n]].set(title = \"Column %s\" %(n+1),\n",
    "                    xlabel = \"Thickness [m]\",\n",
    "                    ylabel = \"Frequency\",\n",
    "                    yticklabels= [])  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc8d780c5a9937c872c18a1cfb350d0c90c85ba2d3f8ee5c7ad7951e9635a330"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
